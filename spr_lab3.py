# -*- coding: utf-8 -*-
"""SPR_Lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iYVS_VT3pkP8tMonHaeiFKiujmN3xDvY
"""

# ===========================================================
#  SPEECH-TO-TEXT ACCESSIBILITY LAB ‚Äî COLAB + VISUALIZATIONS
# ===========================================================

# 0) Setup & Installs
!pip -q install SpeechRecognition openai-whisper vosk pydub librosa soundfile rapidfuzz matplotlib seaborn jiwer
!apt -q -y install ffmpeg

import os, time, json, wave, contextlib, io, warnings
warnings.filterwarnings("ignore")

from google.colab import files
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import librosa, librosa.display
import soundfile as sf

import speech_recognition as sr
import whisper
from vosk import Model, KaldiRecognizer
from rapidfuzz.fuzz import partial_ratio, token_sort_ratio, ratio
from jiwer import cer, wer

# -------------------------------
# 1) Upload audio
# -------------------------------
print("üìÅ Upload an audio file (.wav/.mp3/.flac recommended)")
uploaded = files.upload()
if not uploaded:
    raise SystemExit("No file uploaded.")
audio_file = list(uploaded.keys())[0]
print(f"‚úÖ Uploaded: {audio_file}")

# -------------------------------
# 2) Prep: convert a Vosk-friendly WAV (mono 16k PCM16)
# -------------------------------
converted_file = "converted.wav"
!ffmpeg -loglevel error -y -i "$audio_file" -ac 1 -ar 16000 -sample_fmt s16 "$converted_file"
print("üîß Converted to:", converted_file, "(mono, 16kHz, PCM16)")

# -------------------------------
# 3) Helper: get audio duration (sec)
# -------------------------------
def get_duration_sec(path):
    try:
        with contextlib.closing(wave.open(path, 'r')) as wf:
            frames = wf.getnframes()
            rate = wf.getframerate()
            return frames / float(rate)
    except:
        # fallback via librosa
        y, sr_ = librosa.load(path, sr=None)
        return len(y) / sr_

orig_duration = get_duration_sec(converted_file)
print(f"‚è±Ô∏è Audio duration: {orig_duration:.2f} s")

# -------------------------------
# 4) Run Engines with timing + errors
# -------------------------------
results = {}
latencies = {}

# 4a) Google (online)
print("\nüé§ Google Speech Recognition:")
rec = sr.Recognizer()
with sr.AudioFile(audio_file) as source:
    audio_data = rec.record(source)

t0 = time.time()
try:
    google_text = rec.recognize_google(audio_data)
    results["Google API"] = google_text.strip()
    print("‚úÖ Google:", results["Google API"])
except sr.UnknownValueError:
    results["Google API"] = "[ERROR] Could not understand audio"
    print("‚ö†Ô∏è Google: Could not understand audio")
except sr.RequestError as e:
    results["Google API"] = f"[ERROR] API request failed: {e}"
    print("üö´ Google API error:", e)
latencies["Google API"] = time.time() - t0

# 4b) Whisper (offline)
print("\nü§ñ Whisper (offline) ‚Äî loading model‚Ä¶")
t0_model = time.time()
w_model = whisper.load_model("base")  # tiny/base/small/medium/large
t1_model = time.time()

print("üîé Whisper transcribing...")
t0 = time.time()
try:
    w_res = w_model.transcribe(audio_file)
    whisper_text = (w_res.get("text") or "").strip()
    if whisper_text:
        results["Whisper"] = whisper_text
        print("‚úÖ Whisper:", whisper_text)
    else:
        results["Whisper"] = "[ERROR] Empty transcription"
        print("‚ö†Ô∏è Whisper returned empty text")
except Exception as e:
    results["Whisper"] = f"[ERROR] {e}"
    print("üí• Whisper error:", e)
latencies["Whisper"] = time.time() - t0
whisper_model_load = t1_model - t0_model

# 4c) Vosk (offline)
print("\nüì¶ Vosk (offline) ‚Äî downloading model (first time only)‚Ä¶")
!wget -q https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip -O vosk-model.zip
!unzip -q -o vosk-model.zip
vosk_dir = "vosk-model-small-en-us-0.15"

print("üîé Vosk transcribing...")
t0 = time.time()
try:
    wf = wave.open(converted_file, "rb")
    recognizer = KaldiRecognizer(Model(vosk_dir), wf.getframerate())
    recognizer.SetWords(True)
    pieces = []
    while True:
        data = wf.readframes(4000)
        if len(data) == 0:
            break
        if recognizer.AcceptWaveform(data):
            j = json.loads(recognizer.Result())
            pieces.append(j.get("text", ""))
    final = json.loads(recognizer.FinalResult()).get("text", "")
    vosk_text = (" ".join([p for p in pieces if p]) + " " + final).strip()
    if vosk_text:
        results["Vosk"] = vosk_text
        print("‚úÖ Vosk:", vosk_text)
    else:
        results["Vosk"] = "[ERROR] Empty transcription"
        print("‚ö†Ô∏è Vosk returned empty text")
except Exception as e:
    results["Vosk"] = f"[ERROR] {e}"
    print("üí• Vosk error:", e)
latencies["Vosk"] = time.time() - t0

# -------------------------------
# 5) Metrics & Comparison Table
# -------------------------------
def clean_for_metrics(s):
    if s is None or s.startswith("[ERROR]"):
        return ""
    return s.lower().strip()

engines = ["Google API", "Whisper", "Vosk"]
texts   = [results.get(e, "") for e in engines]

df = pd.DataFrame({
    "Engine": engines,
    "Recognized Text": texts,
    "Word Count": [len(clean_for_metrics(t).split()) for t in texts],
    "Char Count": [len(clean_for_metrics(t)) for t in texts],
    "Latency (s)": [latencies.get(e, np.nan) for e in engines]
})

# Use Whisper as pseudo-reference for WER/CER (no ground truth provided)
ref = clean_for_metrics(results.get("Whisper", ""))
df["WER vs Whisper"] = [wer(ref, clean_for_metrics(t)) if ref and e!="Whisper" else 0.0 for e,t in zip(engines, texts)]
df["CER vs Whisper"] = [cer(ref, clean_for_metrics(t)) if ref and e!="Whisper" else 0.0 for e,t in zip(engines, texts)]

# Pairwise similarity (RapidFuzz ratio 0..100)
sim = np.zeros((len(engines), len(engines)))
for i, ei in enumerate(engines):
    for j, ej in enumerate(engines):
        sim[i, j] = ratio(clean_for_metrics(results.get(ei, "")), clean_for_metrics(results.get(ej, "")))

print("\nüìä COMPARISON TABLE")
display(df)

df.to_csv("comparison_results.csv", index=False)
print("üíæ Saved: comparison_results.csv")

# -------------------------------
# 6) Visualizations
# -------------------------------

# 6a) Waveform + Spectrogram
y, sr_native = librosa.load(audio_file, sr=None)
plt.figure(figsize=(12, 3))
librosa.display.waveshow(y, sr=sr_native)
plt.title("Waveform")
plt.xlabel("Time (s)")
plt.ylabel("Amplitude")
plt.tight_layout()
plt.savefig("viz_waveform.png", dpi=150)
plt.show()

plt.figure(figsize=(12, 4))
S = librosa.feature.melspectrogram(y=y, sr=sr_native, n_mels=64)
S_dB = librosa.power_to_db(S, ref=np.max)
librosa.display.specshow(S_dB, sr=sr_native, x_axis='time', y_axis='mel')
plt.colorbar(format="%+2.0f dB")
plt.title("Mel Spectrogram")
plt.tight_layout()
plt.savefig("viz_spectrogram.png", dpi=150)
plt.show()

# 6b) Bar chart ‚Äî Word Count
plt.figure(figsize=(7,4))
plt.bar(df["Engine"], df["Word Count"])
plt.title("Word Count per Engine")
plt.ylabel("Words")
plt.tight_layout()
plt.savefig("viz_wordcount.png", dpi=150)
plt.show()

# 6c) Bar chart ‚Äî Latency
plt.figure(figsize=(7,4))
plt.bar(df["Engine"], df["Latency (s)"])
plt.title("Latency per Engine (lower is better)")
plt.ylabel("Seconds")
plt.tight_layout()
plt.savefig("viz_latency.png", dpi=150)
plt.show()

# 6d) Heatmap ‚Äî Pairwise Similarity
plt.figure(figsize=(5,4))
sns.heatmap(sim, annot=True, fmt=".0f", xticklabels=engines, yticklabels=engines)
plt.title("Transcript Similarity (RapidFuzz Ratio 0‚Äì100)")
plt.tight_layout()
plt.savefig("viz_similarity.png", dpi=150)
plt.show()

print("\nüñºÔ∏è Saved figures:")
print("- viz_waveform.png")
print("- viz_spectrogram.png")
print("- viz_wordcount.png")
print("- viz_latency.png")
print("- viz_similarity.png")

# -------------------------------
# 7) Friendly Summary (paste into lab)
# -------------------------------
def safe_preview(s, n=140):
    s = s.replace("\n"," ").strip()
    return s if len(s)<=n else s[:n]+"..."

print("\n===== SUMMARY =====")
for e in engines:
    print(f"{e:>12}: {safe_preview(results.get(e,'(no text)'))}")
print("\nModel load time (Whisper): %.2fs" % whisper_model_load)
print("Latency (s):", {k: round(v, 2) for k,v in latencies.items()})
print("===================\n")